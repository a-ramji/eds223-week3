## introduction
In this lab, we'll explore the basics of spatial and geometry operations on vector data in R using the **sf** package.
The following materials are modified from [Chapter 4](https://geocompr.robinlovelace.net/spatial-operations.html) and [Chapter 5](https://geocompr.robinlovelace.net/geometry-operations.html)of Geocomputation with R by Rovin Lovelace.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## prerequisites

```{r install, include=TRUE, warning=FALSE}
rm(list = ls())
library(sf)
library(spData)
library(tmap)
library(tidyverse)
library(rmapshaper)
library(smoothr)
```

## spatial subsetting
Spatial subsetting is the process of converting a spatial object into a new object containing only the features that *relate* in space to another object. This is analogous the attribute subsetting that we covered last week. There are many ways to spatially subset in R, so we will explore a few.

Let's start by going back to the New Zealand datasets and find all the high points in the state of Canterbury.  

The command

```{r include=TRUE}
canterbury <- nz |> 
  filter(Name == "Canterbury")

c_height <- nz_height[canterbury, ] # finds where geometries intersect between nz_height and those specified in our canterbury object (nz subset that contains geometries)


tm_shape(nz) +
  tm_polygons() +
  tm_shape(canterbury) + # subset of nz
  tm_polygons(fill = "blue") +
  tm_shape(c_height) + # subset of nz_height
  tm_dots(fill = "red")

```

The default is to subset to features that intersect, but we can use other operations, including finding features that do not intersect.

Disjoint

```{r include=TRUE}
outside_height <- nz_height[canterbury, , op = st_disjoint]

tm_shape(nz) +
  tm_polygons() +
  tm_shape(canterbury) +
  tm_polygons(fill = "blue") +
  tm_shape(outside_height) +
  tm_dots(fill = "red")
```

We can perform the same operations using topological operators. These operators return matrices testing the relationship between features. 

```{r include=TRUE}
# sparse geometry binary predicate list
sel_sgbp <- st_intersects(x = nz_height, y = canterbury)

sel_logical <- lengths(sel_sgbp) > 0 # returns list of booleans/ TRUE/FALSE
# sometimes knowing the number of things that intersect can be helpful
# count up the trues and falses to get that number
# if you have a lot of data intersecting the geometries themselves, computationally expensive, if you don't need the geometries to stick around, just counting up the trues and falses can be helpful


c_height2 <- nz_height[sel_logical,]

```
output shows "Sparse geometry binary predicate list of length 101, where the predicate was
`intersects'" 
- nz height has 101 objects in it, so list of length 101. getting an answer for every single item in our original object. "binary predicate" - its basically giving us a T/F of whether it's in it or not (isn't reported as a logical, so we need to turn it into a logical based on the lengths of each item in the object). 

We can also use the **st_filter** function in *sf*

st_intersection example
```{r}
c_height3 <- st_intersection(x = nz_height, y = canterbury)
# returns warning: Warning: attribute variables are assumed to be spatially constant throughout all geometries

# note in environment: 70 obs of 9 variables, rather than 3 


# like a join() function
# copied in the variables from canterbury (7 variables), matched to the variables in nz_height (t50_fid, elevation, geometry)


# contextually, doesn't make sense for each of those rows to have all of those extra variables about median income, etc. 


# cases where if you had 2 polygons and they intersect
# working with EJ Screen data, census block groups (county, divided up in weird ways), taking historical redlining data, same area, divided up in slightly different ways


# redlining data has grades associated with it
# comparing air quality from EJ Screen to redlining grade, use an intersection to bring those together
```


```{r include=TRUE}
nz_height |> 
  st_filter(y = canterbury, .predicate = st_intersects)



```

We can change the predicate option to test subset to features that don't intersect

```{r include=TRUE}
# can set predicate to disjoint

```
## buffers

Buffers create polygons representing a set distance from a feature.

	documentation on dist argument in st_buffer:
numeric; buffer distance for all, or for each of the elements in x; in case dist is a units object, it should be convertible to arc_degree if x has geographic coordinates, and to st_crs(x)$units otherwise

```{r include=TRUE}
# seine
seine_buffer <- st_buffer(seine, dist = 5000) # 5 km 
# 


tm_shape(seine) +
  tm_lines()

tm_shape(seine_buffer) +
  tm_lines()
```

## unions
As we saw in the last lab, we can **spatially aggregate** without explicitly asking R to do so.

```{r include=TRUE}
test <- world |> 
  group_by(continent) |> 
  summarize(population = sum(pop, na.rm = TRUE))

us_west <-us_states |> 
  filter(REGION == "West")
  
us_west_union <- st_union(us_west)

tm_shape(us_west_union) +
  tm_polygons()


texas <- us_states |> 
  filter(NAME == "Texas")

texas_union <- st_union(us_west_union, texas)

tm_shape(texas_union) +
  tm_polygons()

```

What is going on here? Behind the scenes, **summarize()** is using **st_union()** to dissolve the boundaries.

```{r include=TRUE}


```

**st_union()** can also take 2 geometries and unite them.

```{r include=TRUE}

```

## spatial joining
Where attribute joining depends on both data sets sharing a 'key' variable, spatial joining uses the same concept but depends on spatial relationships between data sets. 

Let's test this out by creating 50 points randomly distributed across the world and finding out what countries they fall in.

note: st_crs() is a helpful way to check what coordinate reference system a dataset is using/built on 


st_transform() to reproject data (in chpater 7)

transform data by taking your object, telling it the new crs with crs = st_crs(other data set you're working with!)

ex:
st_transform(nz_height, crs = st_crs(world))

```{r include=TRUE}
set.seed(2018)

# bounding box
bb <- st_bbox(world)
# xmin, ymin, xmax, ymax

# random uniform distribution
random_df <- data.frame(
  x = runif(n = 10, min = bb[1], max = bb[3]),   # set the bounds based on our bounding box index positions for xmin, xmax, etc. 
  y = runif(n = 10, min = bb[2], max = bb[4])
  
)

random_points <- random_df |> 
  st_as_sf(coords = c("x", "y")) |>  # turn lat and long into spatial object
  st_set_crs("EPSG:4326")

tm_shape(world) +
  tm_fill() +
  tm_shape(random_points) +
  tm_dots(fill = "red")

```

Let's first use spatial subsetting to find just the countries that contain random points.

```{r include=TRUE}

world_random <- world[random_points, ] # returns the info for each country/location our points fall inside

```

Now let's perform a spatial join to add the info from each country that a point falls into onto the point dataset.

```{r include=TRUE}

random_joined <- st_join(random_points, world)
# NAs are in the ocean so they don't have country information associated with them
# figured out the intersection, and joined based on that data

```

By default, **st_join** performs a left join. We change this and instead perform an inner join.

```{r include=TRUE}

?st_join

```

## non-overlapping joins
Sometimes we might want join geographic datasets that are strongly related, but do not have overlapping geometries. To demonstrate this, let's look at data on cycle hire points in London.

skipped in class :( 

```{r include=TRUE}

```

We can check if any of these points overlap.
```{r include=TRUE}

```

Let's say we need to join the 'capacity' variable in 'cycle_hire_osm' onto the official 'target' data in 'cycle_hire'. The simplest method is using the topological operator **st_is_within_distance()**.

```{r include=TRUE}

```

Now, we'd like to add the values from 'cycle_hire_osm' onto the 'cycle_hire' points.
```{r include=TRUE}

```
Note: the number of rows of the join is larger than the number of rows in the original dataset. Why? Because some points in 'cycle_hire' were within 20 meters of multiple points in 'cycle_hire_osm'. If we wanted to aggregate so we have just one value per original point, we can use the aggregation methods from last week.

```{r include=TRUE}

```

## spatial aggregation
Similar to attribute data aggregation, spatial data aggregation condenses data (we end up with fewer rows than we started with).

Let's say we wanted to find the average height of high point in each region of New Zealand. We could use the **aggregate** function in base R.

```{r include=TRUE}
nz_agg <- aggregate(x = nz_height, # took all of our points
                    by = nz, # and summarized them into these regions
                    FUN = mean)
# debugging! 
# we have the right number of rows (16), but not sure if we have the right values (unclear)

tm_shape(nz_agg) +
  tm_polygons(fill = "elevation")

# lost the region names somehow, but still ended up with what we wanted

```
The result of this is an object with the same geometries as the aggregating feature data set (in this case 'nz').

```{r include=TRUE}

```

We could also use a **sf**/**dplyr** approach.
```{r include=TRUE}

st_join(nz, nz_height) |> 
  group_by(Name) |> 
  summarise(elevation = mean(elevation, na.rm = TRUE))

# note: This actually kept the region name associated with the geometries! 

```

## joining incongruent layers
We might want to aggregate data to geometries that are not congruent (i.e. their boundaries don't line up). This causes issues when we think about how to summarize associated values.
### important for HW2!
- for when there's missmatch in the areas we're trying to aggregate (very common!)

```{r include=TRUE}
head(incongruent)

head(aggregating_zones)

tm_shape(incongruent) +
  tm_polygons() +
  tm_shape(aggregating_zones) +
  tm_borders(col = "red")

# weight based on the relative amount of area that the polygons are in 

```

The simplest method for dealing with this is using area weighted spatial interpolation which transfers values from the 'incongruent' object to a new column in 'aggregating_zones' in proportion with the area of overlap. 

documentation on st_interpolate_aw:

extensive:
	
logical; if TRUE, the attribute variables are assumed to be spatially extensive (like population) and the sum is preserved, otherwise, spatially intensive (like population density) and the mean is preserved.

```{r include=TRUE}
# head(incongruent) # transfer "values"

iv <- incongruent["value"] # sticky geometries

# area-weighted interpolation
agg_aw <- st_interpolate_aw(iv, aggregating_zones, extensive = TRUE) 
# warning is basically saying that we're assuming that attributes are evenly distributed over the entire area of a polygon (not great, but important to keep in mind for our analysis -- might not be appropriate, should think about it more critically)



tm_shape(agg_aw) +
  tm_fill(fill = "value")
 
```
## centroids
Centroids identify the center of a spatial feature. Similar to taking an average, there are many ways to compute a centroid. The most common is the *geographic* centroid.

- gives you points to display as opposed to polygons 
- the center of mass of an object
- if you took a polygon, rotated it, and attached a string with a weight on it, where would that line fall through?
- that point in the center is what the centroid in 

```{r include=TRUE}
nz_centroids <-  st_centroid(nz)

tm_shape(nz) +
  tm_fill() +
  tm_shape(nz_centroids) +
  tm_dots(fill = "blue")

# think about donut problem -- sometimes the centroid isn't even in the area of interest because of how the area is distributed

```

Sometimes centroids fall outside of the boundaries of the objects they were created from. In the case where we need them to fall inside of the feature, we can use *point on surface* methods.


```{r include=TRUE}

```

## distance relationships
While topological relationships are binary (features either intersect or don't), distance relationships are continuous. 

We can use the following to find the distance between the highest point in NZ and the centroid of the Canterbury region.

```{r include=TRUE}


```
Note: this function returns distances with units (yay!) and as a matrix, meaning we could find the distance between many locations at once.

## simplification
Simplification generalizes vector data (polygons and lines) to assist with plotting and reducing the amount of memory, disk space, and network bandwidth to handle a dataset.

Let's try simplifying the US states using the Douglas-Peucker algorithm. GEOS assumes a projected CRS, so we first need to project the data, in this case into the US National Atlas Equal Area (epsg = 2163)

```{r include=TRUE}

```

To preserve the states' topology let's use a simplify function from **rmapshaper** which uses Visalingam's algorithm.

```{r include=TRUE}

```

Instead of simplifying, we could try smoothing using Gaussian kernel regression.

```{r include=TRUE}

```



### Notes: Convex hulls:
- ex: you have a stand of trees (points), can turn that into a polygon -- connects up the points along the outside
- simple way of creating an area
- areas that animals occupy -- GPS data on where animals are moving, draw a convex hull around the observations
